nohup: ignoring input
DataParallel(
  (module): MobileNetV2(
    (features): Sequential(
      (0): Sequential(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace)
      )
      (1): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): InvertedResidual(
        (conv): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace)
          (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU6(inplace)
          (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): Sequential(
        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace)
      )
    )
    (classifier): Sequential(
      (0): Dropout(p=0.2)
      (1): Linear(in_features=1280, out_features=3, bias=True)
    )
  )
)
['BLACK', 'WHITE', 'YELLOW']
Epoch 0/29
----------
 train Loss: 1.0966 Acc: 0.3557
 val Loss: 1.0844 Acc: 0.4101

Epoch 1/29
----------
 train Loss: 0.9683 Acc: 0.5112
 val Loss: 0.8950 Acc: 0.5728

Epoch 2/29
----------
 train Loss: 0.7894 Acc: 0.6292
 val Loss: 0.8300 Acc: 0.6143

Epoch 3/29
----------
 train Loss: 0.6658 Acc: 0.7104
 val Loss: 0.6588 Acc: 0.7217

Epoch 4/29
----------
 train Loss: 0.5604 Acc: 0.7686
 val Loss: 0.5263 Acc: 0.7853

Epoch 5/29
----------
 train Loss: 0.4778 Acc: 0.8051
 val Loss: 0.4880 Acc: 0.8022

Epoch 6/29
----------
 train Loss: 0.4246 Acc: 0.8324
 val Loss: 0.4220 Acc: 0.8331

Epoch 7/29
----------
 train Loss: 0.3747 Acc: 0.8530
 val Loss: 0.4751 Acc: 0.8124

Epoch 8/29
----------
 train Loss: 0.3399 Acc: 0.8648
 val Loss: 0.3743 Acc: 0.8526

Epoch 9/29
----------
 train Loss: 0.2999 Acc: 0.8826
 val Loss: 0.3553 Acc: 0.8578

Epoch 10/29
----------
 train Loss: 0.2693 Acc: 0.8947
 val Loss: 0.3513 Acc: 0.8652

Epoch 11/29
----------
 train Loss: 0.2396 Acc: 0.9077
 val Loss: 0.3847 Acc: 0.8505

Epoch 12/29
----------
 train Loss: 0.2187 Acc: 0.9145
 val Loss: 0.3705 Acc: 0.8646

Epoch 13/29
----------
 train Loss: 0.1986 Acc: 0.9237
 val Loss: 0.3265 Acc: 0.8800

Epoch 14/29
----------
 train Loss: 0.1816 Acc: 0.9301
 val Loss: 0.3145 Acc: 0.8839

Epoch 15/29
----------
 train Loss: 0.1596 Acc: 0.9388
 val Loss: 0.3255 Acc: 0.8841

Epoch 16/29
----------
 train Loss: 0.1450 Acc: 0.9435
 val Loss: 0.3631 Acc: 0.8714

Epoch 17/29
----------
 train Loss: 0.1291 Acc: 0.9512
 val Loss: 0.3504 Acc: 0.8761

Epoch 18/29
----------
 train Loss: 0.1143 Acc: 0.9573
 val Loss: 0.3500 Acc: 0.8852

Epoch 19/29
----------
 train Loss: 0.1037 Acc: 0.9609
 val Loss: 0.3447 Acc: 0.8887

Epoch 20/29
----------
 train Loss: 0.0882 Acc: 0.9679
 val Loss: 0.4331 Acc: 0.8662

Epoch 21/29
----------
 train Loss: 0.0846 Acc: 0.9695
 val Loss: 0.3799 Acc: 0.8843

Epoch 22/29
----------
 train Loss: 0.0693 Acc: 0.9755
 val Loss: 0.3499 Acc: 0.8898

Epoch 23/29
----------
 train Loss: 0.0635 Acc: 0.9781
 val Loss: 0.3841 Acc: 0.8828

Epoch 24/29
----------
 train Loss: 0.0576 Acc: 0.9784
 val Loss: 0.3832 Acc: 0.8872

Epoch 25/29
----------
 train Loss: 0.0372 Acc: 0.9885
 val Loss: 0.3583 Acc: 0.8934

Epoch 26/29
----------
 train Loss: 0.0264 Acc: 0.9939
 val Loss: 0.3584 Acc: 0.8945

Epoch 27/29
----------
 train Loss: 0.0210 Acc: 0.9952
 val Loss: 0.3603 Acc: 0.8963

Epoch 28/29
----------
 train Loss: 0.0177 Acc: 0.9967
 val Loss: 0.3586 Acc: 0.8995

Epoch 29/29
----------
 train Loss: 0.0151 Acc: 0.9979
 val Loss: 0.3650 Acc: 0.8958

Train complete in 27m 30s
Best val Acc: 0.899545
