nohup: ignoring input
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=3, bias=True)
  )
)
['BLACK', 'WHITE', 'YELLOW']
Epoch 0/29
----------
 train Loss: 0.0815 Acc: 0.9740
 val Loss: 0.0691 Acc: 0.9779

Epoch 1/29
----------
 train Loss: 0.0555 Acc: 0.9845
 val Loss: 0.0704 Acc: 0.9776

Epoch 2/29
----------
 train Loss: 0.0394 Acc: 0.9902
 val Loss: 0.0693 Acc: 0.9771

Epoch 3/29
----------
 train Loss: 0.0275 Acc: 0.9944
 val Loss: 0.0693 Acc: 0.9781

Epoch 4/29
----------
 train Loss: 0.0183 Acc: 0.9970
 val Loss: 0.0705 Acc: 0.9785

Epoch 5/29
----------
 train Loss: 0.0137 Acc: 0.9979
 val Loss: 0.0764 Acc: 0.9758

Epoch 6/29
----------
 train Loss: 0.0111 Acc: 0.9987
 val Loss: 0.0733 Acc: 0.9781

Epoch 7/29
----------
 train Loss: 0.0086 Acc: 0.9990
 val Loss: 0.0767 Acc: 0.9779

Epoch 8/29
----------
 train Loss: 0.0064 Acc: 0.9997
 val Loss: 0.0765 Acc: 0.9768

Epoch 9/29
----------
 train Loss: 0.0051 Acc: 0.9997
 val Loss: 0.0764 Acc: 0.9784

Epoch 10/29
----------
 train Loss: 0.0044 Acc: 0.9997
 val Loss: 0.0850 Acc: 0.9759

Epoch 11/29
----------
 train Loss: 0.0032 Acc: 1.0000
 val Loss: 0.0806 Acc: 0.9777

Epoch 12/29
----------
 train Loss: 0.0030 Acc: 1.0000
 val Loss: 0.0808 Acc: 0.9777

Epoch 13/29
----------
 train Loss: 0.0031 Acc: 0.9999
 val Loss: 0.0827 Acc: 0.9774

Epoch 14/29
----------
 train Loss: 0.0029 Acc: 0.9999
 val Loss: 0.0813 Acc: 0.9785

Epoch 15/29
----------
 train Loss: 0.0027 Acc: 0.9998
 val Loss: 0.0820 Acc: 0.9785

Epoch 16/29
----------
 train Loss: 0.0022 Acc: 1.0000
 val Loss: 0.0849 Acc: 0.9771

Epoch 17/29
----------
 train Loss: 0.0016 Acc: 1.0000
 val Loss: 0.0850 Acc: 0.9776

Epoch 18/29
----------
 train Loss: 0.0016 Acc: 1.0000
 val Loss: 0.0861 Acc: 0.9776

Epoch 19/29
----------
 train Loss: 0.0019 Acc: 0.9999
 val Loss: 0.0859 Acc: 0.9763

Epoch 20/29
----------
 train Loss: 0.0017 Acc: 1.0000
 val Loss: 0.0863 Acc: 0.9781

Epoch 21/29
----------
 train Loss: 0.0014 Acc: 1.0000
 val Loss: 0.0877 Acc: 0.9779

Epoch 22/29
----------
 train Loss: 0.0013 Acc: 1.0000
 val Loss: 0.0890 Acc: 0.9763

Epoch 23/29
----------
 train Loss: 0.0014 Acc: 1.0000
 val Loss: 0.0903 Acc: 0.9777

Epoch 24/29
----------
 train Loss: 0.0013 Acc: 1.0000
 val Loss: 0.0902 Acc: 0.9776

Epoch 25/29
----------
 train Loss: 0.0009 Acc: 1.0000
 val Loss: 0.0921 Acc: 0.9772

Epoch 26/29
----------
 train Loss: 0.0013 Acc: 0.9999
 val Loss: 0.0930 Acc: 0.9769

Epoch 27/29
----------
 train Loss: 0.0009 Acc: 1.0000
 val Loss: 0.0903 Acc: 0.9774

Epoch 28/29
----------
 train Loss: 0.0010 Acc: 1.0000
 val Loss: 0.0911 Acc: 0.9771

Epoch 29/29
----------
 train Loss: 0.0010 Acc: 1.0000
 val Loss: 0.0906 Acc: 0.9774

Train complete in 17m 15s
Best val Acc: 0.978544
