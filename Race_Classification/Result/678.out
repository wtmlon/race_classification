nohup: ignoring input
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): SEBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SELayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=64, out_features=4, bias=False)
            (1): ReLU(inplace)
            (2): Linear(in_features=4, out_features=64, bias=False)
            (3): Sigmoid()
          )
        )
      )
      (1): SEBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SELayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=64, out_features=4, bias=False)
            (1): ReLU(inplace)
            (2): Linear(in_features=4, out_features=64, bias=False)
            (3): Sigmoid()
          )
        )
      )
    )
    (layer2): Sequential(
      (0): SEBasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SELayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=128, out_features=8, bias=False)
            (1): ReLU(inplace)
            (2): Linear(in_features=8, out_features=128, bias=False)
            (3): Sigmoid()
          )
        )
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): SEBasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SELayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=128, out_features=8, bias=False)
            (1): ReLU(inplace)
            (2): Linear(in_features=8, out_features=128, bias=False)
            (3): Sigmoid()
          )
        )
      )
    )
    (layer3): Sequential(
      (0): SEBasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SELayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=256, out_features=16, bias=False)
            (1): ReLU(inplace)
            (2): Linear(in_features=16, out_features=256, bias=False)
            (3): Sigmoid()
          )
        )
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): SEBasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SELayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=256, out_features=16, bias=False)
            (1): ReLU(inplace)
            (2): Linear(in_features=16, out_features=256, bias=False)
            (3): Sigmoid()
          )
        )
      )
    )
    (layer4): Sequential(
      (0): SEBasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SELayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=512, out_features=32, bias=False)
            (1): ReLU(inplace)
            (2): Linear(in_features=32, out_features=512, bias=False)
            (3): Sigmoid()
          )
        )
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): SEBasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SELayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=512, out_features=32, bias=False)
            (1): ReLU(inplace)
            (2): Linear(in_features=32, out_features=512, bias=False)
            (3): Sigmoid()
          )
        )
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=512, out_features=3, bias=True)
  )
)
['BLACK', 'WHITE', 'YELLOW']
Epoch 0/29
----------
 train Loss: 1.0042 Acc: 0.5080
 val Loss: 0.9037 Acc: 0.5897

Epoch 1/29
----------
 train Loss: 0.8104 Acc: 0.6502
 val Loss: 0.7256 Acc: 0.6890

Epoch 2/29
----------
 train Loss: 0.6543 Acc: 0.7305
 val Loss: 0.6017 Acc: 0.7495

Epoch 3/29
----------
 train Loss: 0.5174 Acc: 0.7954
 val Loss: 0.5948 Acc: 0.7375

Epoch 4/29
----------
 train Loss: 0.4178 Acc: 0.8338
 val Loss: 0.5404 Acc: 0.7650

Epoch 5/29
----------
 train Loss: 0.3533 Acc: 0.8609
 val Loss: 0.3539 Acc: 0.8599

Epoch 6/29
----------
 train Loss: 0.3050 Acc: 0.8830
 val Loss: 0.4597 Acc: 0.8207

Epoch 7/29
----------
 train Loss: 0.2747 Acc: 0.8943
 val Loss: 0.3051 Acc: 0.8797

Epoch 8/29
----------
 train Loss: 0.2387 Acc: 0.9081
 val Loss: 0.3241 Acc: 0.8730

Epoch 9/29
----------
 train Loss: 0.2124 Acc: 0.9182
 val Loss: 0.3535 Acc: 0.8612

Epoch 10/29
----------
 train Loss: 0.1919 Acc: 0.9284
 val Loss: 0.2819 Acc: 0.8877

Epoch 11/29
----------
 train Loss: 0.1685 Acc: 0.9385
 val Loss: 0.3175 Acc: 0.8797

Epoch 12/29
----------
 train Loss: 0.1454 Acc: 0.9470
 val Loss: 0.2745 Acc: 0.8927

Epoch 13/29
----------
 train Loss: 0.1247 Acc: 0.9562
 val Loss: 0.2687 Acc: 0.8969

Epoch 14/29
----------
 train Loss: 0.1092 Acc: 0.9627
 val Loss: 0.3294 Acc: 0.8887

Epoch 15/29
----------
 train Loss: 0.0933 Acc: 0.9703
 val Loss: 0.2672 Acc: 0.9033

Epoch 16/29
----------
 train Loss: 0.0770 Acc: 0.9778
 val Loss: 0.3297 Acc: 0.8857

Epoch 17/29
----------
 train Loss: 0.0628 Acc: 0.9828
 val Loss: 0.2900 Acc: 0.8976

Epoch 18/29
----------
 train Loss: 0.0514 Acc: 0.9879
 val Loss: 0.2621 Acc: 0.9101

Epoch 19/29
----------
 train Loss: 0.0435 Acc: 0.9905
 val Loss: 0.4120 Acc: 0.8817

Epoch 20/29
----------
 train Loss: 0.0313 Acc: 0.9952
 val Loss: 0.3087 Acc: 0.9030

Epoch 21/29
----------
 train Loss: 0.0269 Acc: 0.9964
 val Loss: 0.3088 Acc: 0.8968

Epoch 22/29
----------
 train Loss: 0.0193 Acc: 0.9985
 val Loss: 0.2914 Acc: 0.9065

Epoch 23/29
----------
 train Loss: 0.0155 Acc: 0.9990
 val Loss: 0.2816 Acc: 0.9064

Epoch 24/29
----------
 train Loss: 0.0142 Acc: 0.9992
 val Loss: 0.2943 Acc: 0.9099

Epoch 25/29
----------
 train Loss: 0.0106 Acc: 0.9997
 val Loss: 0.2833 Acc: 0.9108

Epoch 26/29
----------
 train Loss: 0.0099 Acc: 0.9998
 val Loss: 0.2824 Acc: 0.9121

Epoch 27/29
----------
 train Loss: 0.0091 Acc: 0.9998
 val Loss: 0.2831 Acc: 0.9098

Epoch 28/29
----------
 train Loss: 0.0088 Acc: 0.9999
 val Loss: 0.2851 Acc: 0.9122

Epoch 29/29
----------
 train Loss: 0.0086 Acc: 0.9998
 val Loss: 0.2848 Acc: 0.9109

Train complete in 20m 10s
Best val Acc: 0.912224
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fc690a3bb70>>
Traceback (most recent call last):
  File "/home/liuting/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py", line 399, in __del__
    self._shutdown_workers()
  File "/home/liuting/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py", line 378, in _shutdown_workers
    self.worker_result_queue.get()
  File "/usr/lib/python3.5/multiprocessing/queues.py", line 345, in get
    return ForkingPickler.loads(res)
  File "/home/liuting/.local/lib/python3.5/site-packages/torch/multiprocessing/reductions.py", line 151, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.5/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.5/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 494, in Client
    deliver_challenge(c, authkey)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 722, in deliver_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
