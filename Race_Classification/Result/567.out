nohup: ignoring input
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=3, bias=True)
  )
)
['BLACK', 'WHITE', 'YELLOW']
Epoch 0/49
----------
 train Loss: 0.4452 Acc: 0.8153
 val Loss: 0.2558 Acc: 0.8981

Epoch 1/49
----------
 train Loss: 0.2015 Acc: 0.9231
 val Loss: 0.1898 Acc: 0.9282

Epoch 2/49
----------
 train Loss: 0.1397 Acc: 0.9481
 val Loss: 0.1739 Acc: 0.9340

Epoch 3/49
----------
 train Loss: 0.1009 Acc: 0.9643
 val Loss: 0.1550 Acc: 0.9428

Epoch 4/49
----------
 train Loss: 0.0747 Acc: 0.9747
 val Loss: 0.1573 Acc: 0.9447

Epoch 5/49
----------
 train Loss: 0.0551 Acc: 0.9823
 val Loss: 0.1582 Acc: 0.9464

Epoch 6/49
----------
 train Loss: 0.0396 Acc: 0.9888
 val Loss: 0.1578 Acc: 0.9452

Epoch 7/49
----------
 train Loss: 0.0260 Acc: 0.9943
 val Loss: 0.1642 Acc: 0.9460

Epoch 8/49
----------
 train Loss: 0.0195 Acc: 0.9963
 val Loss: 0.1667 Acc: 0.9472

Epoch 9/49
----------
 train Loss: 0.0146 Acc: 0.9975
 val Loss: 0.1747 Acc: 0.9454

Epoch 10/49
----------
 train Loss: 0.0107 Acc: 0.9987
 val Loss: 0.1778 Acc: 0.9472

Epoch 11/49
----------
 train Loss: 0.0083 Acc: 0.9990
 val Loss: 0.1795 Acc: 0.9473

Epoch 12/49
----------
 train Loss: 0.0063 Acc: 0.9996
 val Loss: 0.1804 Acc: 0.9480

Epoch 13/49
----------
 train Loss: 0.0047 Acc: 0.9999
 val Loss: 0.1867 Acc: 0.9468

Epoch 14/49
----------
 train Loss: 0.0048 Acc: 0.9997
 val Loss: 0.1933 Acc: 0.9475

Epoch 15/49
----------
 train Loss: 0.0043 Acc: 0.9996
 val Loss: 0.1919 Acc: 0.9480

Epoch 16/49
----------
 train Loss: 0.0034 Acc: 0.9999
 val Loss: 0.2001 Acc: 0.9477

Epoch 17/49
----------
 train Loss: 0.0027 Acc: 0.9999
 val Loss: 0.1953 Acc: 0.9483

Epoch 18/49
----------
 train Loss: 0.0024 Acc: 1.0000
 val Loss: 0.1976 Acc: 0.9475

Epoch 19/49
----------
 train Loss: 0.0026 Acc: 1.0000
 val Loss: 0.2019 Acc: 0.9483

Epoch 20/49
----------
 train Loss: 0.0022 Acc: 1.0000
 val Loss: 0.2022 Acc: 0.9481

Epoch 21/49
----------
 train Loss: 0.0017 Acc: 1.0000
 val Loss: 0.2063 Acc: 0.9477

Epoch 22/49
----------
 train Loss: 0.0016 Acc: 1.0000
 val Loss: 0.2095 Acc: 0.9490

Epoch 23/49
----------
 train Loss: 0.0016 Acc: 1.0000
 val Loss: 0.2101 Acc: 0.9465

Epoch 24/49
----------
 train Loss: 0.0017 Acc: 0.9998
 val Loss: 0.2149 Acc: 0.9486

Epoch 25/49
----------
 train Loss: 0.0014 Acc: 0.9999
 val Loss: 0.2117 Acc: 0.9481

Epoch 26/49
----------
 train Loss: 0.0017 Acc: 0.9998
 val Loss: 0.2144 Acc: 0.9481

Epoch 27/49
----------
 train Loss: 0.0012 Acc: 1.0000
 val Loss: 0.2147 Acc: 0.9486

Epoch 28/49
----------
 train Loss: 0.0013 Acc: 1.0000
 val Loss: 0.2112 Acc: 0.9475

Epoch 29/49
----------
 train Loss: 0.0012 Acc: 1.0000
 val Loss: 0.2148 Acc: 0.9481

Epoch 30/49
----------
 train Loss: 0.0014 Acc: 0.9999
 val Loss: 0.2117 Acc: 0.9478

Epoch 31/49
----------
 train Loss: 0.0013 Acc: 0.9999
 val Loss: 0.2106 Acc: 0.9491

Epoch 32/49
----------
 train Loss: 0.0013 Acc: 1.0000
 val Loss: 0.2111 Acc: 0.9488

Epoch 33/49
----------
 train Loss: 0.0013 Acc: 1.0000
 val Loss: 0.2115 Acc: 0.9475

Epoch 34/49
----------
 train Loss: 0.0014 Acc: 1.0000
 val Loss: 0.2158 Acc: 0.9459

Epoch 35/49
----------
 train Loss: 0.0011 Acc: 1.0000
 val Loss: 0.2125 Acc: 0.9473

Epoch 36/49
----------
 train Loss: 0.0010 Acc: 1.0000
 val Loss: 0.2155 Acc: 0.9452

Epoch 37/49
----------
 train Loss: 0.0012 Acc: 1.0000
 val Loss: 0.2146 Acc: 0.9481

Epoch 38/49
----------
 train Loss: 0.0014 Acc: 1.0000
 val Loss: 0.2120 Acc: 0.9488

Epoch 39/49
----------
 train Loss: 0.0013 Acc: 1.0000
 val Loss: 0.2144 Acc: 0.9481

Epoch 40/49
----------
 train Loss: 0.0013 Acc: 1.0000
 val Loss: 0.2120 Acc: 0.9488

Epoch 41/49
----------
 train Loss: 0.0012 Acc: 1.0000
 val Loss: 0.2125 Acc: 0.9478

Epoch 42/49
----------
 train Loss: 0.0011 Acc: 1.0000
 val Loss: 0.2142 Acc: 0.9491

Epoch 43/49
----------
 train Loss: 0.0010 Acc: 1.0000
 val Loss: 0.2129 Acc: 0.9481

Epoch 44/49
----------
 train Loss: 0.0012 Acc: 1.0000
 val Loss: 0.2135 Acc: 0.9493

Epoch 45/49
----------
 train Loss: 0.0011 Acc: 1.0000
 val Loss: 0.2140 Acc: 0.9477

Epoch 46/49
----------
 train Loss: 0.0010 Acc: 1.0000
 val Loss: 0.2128 Acc: 0.9475

Epoch 47/49
----------
 train Loss: 0.0011 Acc: 1.0000
 val Loss: 0.2131 Acc: 0.9475

Epoch 48/49
----------
 train Loss: 0.0012 Acc: 1.0000
 val Loss: 0.2150 Acc: 0.9462

Epoch 49/49
----------
 train Loss: 0.0012 Acc: 1.0000
 val Loss: 0.2131 Acc: 0.9477

Train complete in 28m 52s
Best val Acc: 0.949285
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f9114e24dd8>>
Traceback (most recent call last):
  File "/home/liuting/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py", line 399, in __del__
    self._shutdown_workers()
  File "/home/liuting/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py", line 378, in _shutdown_workers
    self.worker_result_queue.get()
  File "/usr/lib/python3.5/multiprocessing/queues.py", line 345, in get
    return ForkingPickler.loads(res)
  File "/home/liuting/.local/lib/python3.5/site-packages/torch/multiprocessing/reductions.py", line 151, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.5/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.5/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 494, in Client
    deliver_challenge(c, authkey)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 722, in deliver_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f9111acaba8>>
Traceback (most recent call last):
  File "/home/liuting/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py", line 399, in __del__
    self._shutdown_workers()
  File "/home/liuting/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py", line 378, in _shutdown_workers
    self.worker_result_queue.get()
  File "/usr/lib/python3.5/multiprocessing/queues.py", line 345, in get
    return ForkingPickler.loads(res)
  File "/home/liuting/.local/lib/python3.5/site-packages/torch/multiprocessing/reductions.py", line 151, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.5/multiprocessing/resource_sharer.py", line 58, in detach
    return reduction.recv_handle(conn)
  File "/usr/lib/python3.5/multiprocessing/reduction.py", line 181, in recv_handle
    return recvfds(s, 1)[0]
  File "/usr/lib/python3.5/multiprocessing/reduction.py", line 152, in recvfds
    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))
ConnectionResetError: [Errno 104] Connection reset by peer
